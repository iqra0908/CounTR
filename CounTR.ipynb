{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18g-os5NURSSyKPNHrViqY5zay8sJH4Gu",
      "authorship_tag": "ABX9TyMYYCGAYVxKF1FdUfLTlULs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iqra0908/CounTR/blob/main/CounTR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ribecw0WwVxP",
        "outputId": "ae9714f2-2f5b-498b-aa2e-b94f5acf96ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Othercomputers/My-MacBook-Air/CounTR\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/Othercomputers/My-MacBook-Air/CounTR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt\n",
        "%pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "%pip install timm==0.3.2\n",
        "%pip install numpy\n",
        "%pip install matplotlib tqdm \n",
        "%pip install tensorboard\n",
        "%pip install scipy\n",
        "%pip install imgaug\n",
        "%pip install opencv-python\n",
        "%pip install hub\n",
        "%pip install Pillow==9.1.0\n",
        "%pip install timm==0.4.5\n",
        "%pip install wandb"
      ],
      "metadata": {
        "id": "hgQHvmIdyCz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "wandb login \"91357f0099db3efd90b404f26b2ba7d8fe9d7fa6\"\n",
        "\n",
        "\n",
        "#wandb login --relogin \"91357f0099db3efd90b404f26b2ba7d8fe9d7fa6\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2skroW7cByCe",
        "outputId": "79b877e1-36c9-488a-ccc1-5dceba2da722"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FltV0hsBDlJK",
        "outputId": "afa606e0-4c85-4887-f561-be141010d198"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miqra-imtiaz09\u001b[0m (\u001b[33maipi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "CUDA_VISIBLE_DEVICES=0 python FSC_pretrain.py \\\n",
        "    --epochs 500 \\\n",
        "    --warmup_epochs 10 \\\n",
        "    --blr 1.5e-4 --weight_decay 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-B2hmBOnAFdu",
        "outputId": "05a8117a-a448-4791-cc8d-0d51a1fa443f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "[19:54:34.367148] job dir: /content/drive/Othercomputers/My-MacBook-Air/CounTR\n",
            "[19:54:34.367366] Namespace(accum_iter=1,\n",
            "anno_file='annotation_FSC147_384.json',\n",
            "batch_size=8,\n",
            "blr=0.00015,\n",
            "data_path='./data/FSC147/',\n",
            "data_split_file='Train_Test_Val_FSC_147.json',\n",
            "device='cuda',\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "distributed=False,\n",
            "epochs=500,\n",
            "gt_dir='gt_density_map_adaptive_384_VarV2',\n",
            "im_dir='images_384_VarV2',\n",
            "local_rank=-1,\n",
            "log_dir='./logs/pre_4_dir',\n",
            "lr=None,\n",
            "mask_ratio=0.5,\n",
            "min_lr=0.0,\n",
            "model='mae_vit_base_patch16',\n",
            "norm_pix_loss=False,\n",
            "num_workers=10,\n",
            "output_dir='./data/out/pre_4_dir',\n",
            "pin_mem=True,\n",
            "resume='./weights/mae_pretrain_vit_base_full.pth',\n",
            "seed=0,\n",
            "start_epoch=0,\n",
            "team='aipi',\n",
            "title='CounTR_pretraining',\n",
            "wandb='CounTR',\n",
            "wandb_id=None,\n",
            "warmup_epochs=10,\n",
            "weight_decay=0.05,\n",
            "world_size=1)\n",
            "[19:54:36.198393] <__main__.TrainData object at 0x7f14a4730e80>\n",
            "[19:54:36.198496] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f137cfd6a60>\n",
            "[19:54:50.937492] Model = MaskedAutoencoderViTNoCT(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (norm): Identity()\n",
            "  )\n",
            "  (blocks): ModuleList(\n",
            "    (0): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (2): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (3): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (4): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (5): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (6): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (7): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (8): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (9): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (10): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (11): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
            "  (decoder_blocks): ModuleList(\n",
            "    (0): Block(\n",
            "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (2): Block(\n",
            "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (3): Block(\n",
            "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (4): Block(\n",
            "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (5): Block(\n",
            "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (6): Block(\n",
            "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (7): Block(\n",
            "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
            ")\n",
            "[19:54:50.938426] base lr: 1.50e-04\n",
            "[19:54:50.938746] actual lr: 4.69e-06\n",
            "[19:54:50.938974] accumulate grad iterations: 1\n",
            "[19:54:50.939228] effective batch size: 8\n",
            "[19:54:50.942830] AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    eps: 1e-08\n",
            "    lr: 4.6875e-06\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    eps: 1e-08\n",
            "    lr: 4.6875e-06\n",
            "    weight_decay: 0.05\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:852: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "wandb: Currently logged in as: iqra-imtiaz09 (aipi). Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.13.9\n",
            "wandb: Run data is saved locally in /content/drive/Othercomputers/My-MacBook-Air/CounTR/wandb/run-20230205_195441-u0kd9vds\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run festive-chrysanthemum-1\n",
            "wandb: ‚≠êÔ∏è View project at https://wandb.ai/aipi/CounTR\n",
            "wandb: üöÄ View run at https://wandb.ai/aipi/CounTR/runs/u0kd9vds\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Traceback (most recent call last):\n",
            "  File \"FSC_pretrain.py\", line 363, in <module>\n",
            "    main(args)\n",
            "  File \"FSC_pretrain.py\", line 231, in main\n",
            "    misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)\n",
            "  File \"/content/drive/Othercomputers/My-MacBook-Air/CounTR/util/misc.py\", line 340, in load_model\n",
            "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 594, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 211, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './weights/mae_pretrain_vit_base_full.pth'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a406833d445c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CUDA_VISIBLE_DEVICES=0 python FSC_pretrain.py \\\\\\n    --epochs 500 \\\\\\n    --warmup_epochs 10 \\\\\\n    --blr 1.5e-4 --weight_decay 0.05\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'CUDA_VISIBLE_DEVICES=0 python FSC_pretrain.py \\\\\\n    --epochs 500 \\\\\\n    --warmup_epochs 10 \\\\\\n    --blr 1.5e-4 --weight_decay 0.05\\n'' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh \n",
        "run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "Nppob3cT1pS8",
        "outputId": "e91e2115-a171-4204-fff6-2a22490c2818"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sh: 1: run.sh: not found\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/content/drive/Othercomputers/My-MacBook-Air/CounTR/demo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run.sh\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'run.sh\\n'' returned non-zero exit status 127."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run -i demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "WSCLnWwjxXbc",
        "outputId": "63ec4b19-89b6-45c0-838e-8ca24a2906ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/content/drive/Othercomputers/My-MacBook-Air/CounTR/demo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0mmodel_without_ddp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./output_allnew_dir/checkpoint-400.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0mmodel_without_ddp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resume checkpoint %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'./output_allnew_dir/checkpoint-400.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output_allnew_dir/checkpoint-400.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install numpy==1.23.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqeMBeBA7vmE",
        "outputId": "ffa42451-fb19-40c8-9596-49b653b59aa0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.23.0\n",
            "  Using cached numpy-1.23.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.1\n",
            "    Uninstalling numpy-1.24.1:\n",
            "      Successfully uninstalled numpy-1.24.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.10.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CUDA_VISIBLE_DEVICES=0 python FSC_pretrain.py \\\n",
        "    --epochs 500 \\\n",
        "    --warmup_epochs 10 \\\n",
        "    --blr 1.5e-4 --weight_decay 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkTBk6EswsxG",
        "outputId": "1b42eba4-7e99-4040-e3d2-52ad3ca013bc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:File `'CUDA_VISIBLE_DEVICES=0.py'` not found.\n"
          ]
        }
      ]
    }
  ]
}